<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="Jelmer Snoeck ">
<meta name="description" content="Originally posted on the Manifold blog
We at Manifold always strive to get the most out of everything we do. For this reason, we continuously evaluate what we’ve done to see if it still holds up to our standards. A while back, we decided to take a deeper look at our infrastructure setup.
In this blog post, we’ll look at the reasons why we moved to Kubernetes and the questions we asked ourselves." />
<meta name="keywords" content="" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="" />
<link rel="canonical" href="/articles/2018/03/migrating-to-kubernetes-with-zero-downtime-the-why-and-how/" />


    <title>
        
            Migrating to Kubernetes with zero downtime: the why and how :: Jelmer Snoeck 
        
    </title>



<link href="https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.5.0/css/flag-icon.min.css" rel="stylesheet"
    type="text/css">



<link rel="stylesheet" href="/main.d1ea4af8fd04fb24a4f8b882ea54bd04eb245427ca4baf527c81a5dab071410b.css">






<meta itemprop="name" content="Migrating to Kubernetes with zero downtime: the why and how">
<meta itemprop="description" content="Originally posted on the Manifold blog
We at Manifold always strive to get the most out of everything we do. For this reason, we continuously evaluate what we’ve done to see if it still holds up to our standards. A while back, we decided to take a deeper look at our infrastructure setup.
In this blog post, we’ll look at the reasons why we moved to Kubernetes and the questions we asked ourselves.">
<meta itemprop="datePublished" content="2018-03-21T00:00:00&#43;00:00" />
<meta itemprop="dateModified" content="2018-03-21T00:00:00&#43;00:00" />
<meta itemprop="wordCount" content="2073">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Migrating to Kubernetes with zero downtime: the why and how"/>
<meta name="twitter:description" content="Originally posted on the Manifold blog
We at Manifold always strive to get the most out of everything we do. For this reason, we continuously evaluate what we’ve done to see if it still holds up to our standards. A while back, we decided to take a deeper look at our infrastructure setup.
In this blog post, we’ll look at the reasons why we moved to Kubernetes and the questions we asked ourselves."/>







    <meta property="article:published_time" content="2018-03-21 00:00:00 &#43;0000 UTC" />








    </head>

    <body class="dark-theme">
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">></span>
            <span class="logo__text">$ cd ~</span>
            <span class="logo__cursor" style=
                  "
                   
                   ">
            </span>
        
    </div>
</a>


        <span class="header__right">
            
                <nav class="menu">
    <ul class="menu__inner"><li><a href="/about/">About</a></li><li><a href="/articles/">Articles</a></li><li><a href="/projects/">Projects</a></li><li><a href="/talks/">Talks</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            

            <span class="theme-toggle unselectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
  3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
  13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"/>
</svg>
</span>
        </span>
    </span>
</header>


            <div class="content">
                
    <main class="post">

        <div class="post-info">
            
            </p>
        </div>

        <article>
            <h2 class="post-title"><a href="/articles/2018/03/migrating-to-kubernetes-with-zero-downtime-the-why-and-how/">Migrating to Kubernetes with zero downtime: the why and how</a></h2>

            

            

            <div class="post-content">
                <p>Originally posted on <a href="https://manifold.co/blog/migrating-to-kubernetes-with-zero-downtime-the-why-and-how-d64ba9a92619">the Manifold blog</a></p>
<p><img src="/static/images/migrate-k8s-000.png" alt="header"></p>
<p>We at <a href="https://www.manifold.co/">Manifold</a> always strive to get the most out of everything we do. For this reason, we continuously evaluate what we’ve done to see if it still holds up to our standards. A while back, we decided to take a deeper look at our infrastructure setup.</p>
<p>In this blog post, we’ll look at the reasons why we moved to <a href="https://kubernetes.io/">Kubernetes</a> and the questions we asked ourselves. We’ll then look at some of the compromises we had to make and why we had to make them. We’ll also have a look how we configured our cluster to achieve our goals.</p>
<p>When we started Manifold, we did what we knew worked well. Use <a href="https://www.terraform.io/">Terraform</a> to deploy containers on <a href="https://aws.amazon.com/ec2/">AWS EC2</a> and expose these through <a href="https://aws.amazon.com/elasticloadbalancing/">ELBs</a>. We found ourselves in a position where we could spend some extra time on building a more mature platform. The initial implementation was very simple to begin with, but we started to see some pain points:</p>
<ul>
<li>
<p>Deploying was slow (~15min)</p>
</li>
<li>
<p>No <a href="https://continuousdelivery.com/">Continuous Delivery</a> meant only the Ops people knew how to deploy</p>
</li>
<li>
<p>Running a single container per instance can become expensive. By increasing <a href="https://containerjournal.com/2017/03/15/dockers-big-differentiator-vms-density/">container density</a>, we could decrease cost</p>
</li>
</ul>
<p>In the past year, <a href="https://trends.google.com/trends/explore?date=today%205-y&amp;q=kubernetes">Kubernetes has become very popular</a>. With the experience the team had, we strongly believed in the future of this new technology. For this reason, we created our first <a href="https://github.com/manifoldco/kubernetes-credentials">Kubernetes Integration</a>. We also started thinking about integrations which would make <a href="https://hackernoon.com/tagged/kubernetes">Kubernetes</a> more accessible. This is where the idea of building <a href="https://heighliner.com/">Heighliner</a> was born.</p>
<p>This leads us to another principle we live by: <a href="https://www.urbandictionary.com/define.php?term=dogfooding%20%28to%20dogfood%29">dogfooding</a>. By using Manifold to build Manifold, we’d know exactly what our users need.</p>
<h2 id="choosing-a-cluster">Choosing a cluster</h2>
<p>The first question we asked ourselves was “where are we going to run this cluster?”. AWS does not offer a Kubernetes solution yet but <a href="https://azure.microsoft.com/en-us/blog/introducing-azure-container-service-aks-managed-kubernetes-and-azure-container-registry-geo-replication/">Azure</a> and <a href="https://cloud.google.com/kubernetes-engine/">Google Cloud Platform</a> do. Do we need to stay within AWS and manage our own cluster or do we want to move everything to another Cloud Provider?</p>
<p>The key questions we wanted answers to were:</p>
<ul>
<li>
<p>Can we create a High Availability cluster on AWS and how easy is it to manage this?</p>
</li>
<li>
<p>How do we connect to our <a href="https://aws.amazon.com/rds/">RDS</a> instance and what will the latency be?</p>
</li>
<li>
<p>What do we do about our <a href="https://aws.amazon.com/kms/">KMS</a> encryption?</p>
</li>
</ul>
<h2 id="high-availability-ha-with-kops">High Availability (HA) with kops</h2>
<p>If we can easily create and manage a cluster within AWS, it would lower the necessity to move providers. The initial tests we did with <a href="https://github.com/kubernetes/kops">kops</a> looked promising and we decided to take it a step further. It’s time to set up a High Availability cluster.</p>
<p>To understand what HA means for Kubernetes, we need to first understand what it means in general.</p>
<blockquote>
<p>The central foundation of a highly available solution is a redundant, reliable storage layer. The number one rule of high-availability is to protect the data. Whatever else happens, whatever catches on fire, if you have the data, you can rebuild. If you lose the data, you’re done.</p>
<ul>
<li><a href="https://kubernetes.io/docs/admin/high-availability/building/">Kubernetes docs</a></li>
</ul>
</blockquote>
<p><img src="/static/images/migrate-k8s-001.png" alt="Kubernetes components in a High Availability configuration"><em>Kubernetes components in a High Availability configuration</em></p>
<p>Within a Kubernetes cluster, this storage layer is<a href="https://coreos.com/etcd/"> etcd</a> and runs on the master instances. etcd is a distributed key/value store which follows the <a href="https://raft.github.io/">Raft consensus algorithm</a> to achieve quorum. Achieving quorum means having a set of servers agreeing on a set of values. To reach this consensus, it needs <em>lower(n/2)+1</em> parties to agree. Therefore we always need an uneven amount of instances with at least 3.</p>
<p>Below, we’ll look at a few possible disruption cases.</p>
<h2 id="tolerating-instance-failure">Tolerating instance failure</h2>
<p>The first scenario we’ll look at is to see what happens when a single instance terminates. Can we recover from this?</p>
<p><img src="/static/images/migrate-k8s-002.gif" alt="Tolerating instance failure"><em>Tolerating instance failure</em></p>
<p>By specifying the amount of nodes we want, kops creates an <a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html">Auto Scaling Group</a> per instance group. This ensures that when an instance terminates, a new one gets created. This allows us to keep consensus across our cluster when we lose an instance.</p>
<h2 id="tolerating-zone-failure">Tolerating zone failure</h2>
<p>Having set up instance failure allows us to tolerate failure of a single machine. But what happens when the whole datacenter is having issues due to a power cut for example? This is where <a href="https://buildazure.com/2017/09/22/azure-regions-and-availability-zones/"><em>Regions</em> and *Availability Zones</a>* come into play.</p>
<p>Let’s look back at our consensus formula: <em>lower(n/2)+1 instances with at least 3 instances</em>. We can translate this to zones, which would result in <em>lower(n/2)+1 zones with at least 3 zones</em>.</p>
<p><img src="/static/images/migrate-k8s-003.png" alt="3 master nodes spread across 2 zones"><em>3 master nodes spread across 2 zones</em></p>
<p><img src="/static/images/migrate-k8s-004.png" alt="3 master nodes spread across 3 zones"><em>3 master nodes spread across 3 zones</em></p>
<p>With kops, this too is simple. By specifying the zones we want to run both our masters and nodes in, we can configure HA at the zone level. This however is where we ran into our first roadblock. For arbitrary reasons, when we started Manifold, we decided to use the <em>us-west-1</em> region. As it turns out, this region only has 2 zones available. This meant that we’d have to find another solution to tolerate zone failure.</p>
<h2 id="tolerating-region-failure-and-beyond">Tolerating region failure (and beyond)</h2>
<p>The main goal was to replicate the existing infrastructure. Our legacy setup did not run across multiple regions, so the new setup didn’t have to either. We do believe that with the help of <a href="https://kubernetes.io/docs/concepts/cluster-administration/federation/">Kubernetes Federation</a>, this will be easier to set up.</p>
<h2 id="internal-networking-with-peering">Internal Networking with Peering</h2>
<p>Because of our regional restrictions, we had to find other ways to tolerate zone failure. One option is to create our cluster in a separate region.</p>
<p>Each region runs its own separated network. This means that we can’t just use resources from one region in the other. For this, we looked into <a href="https://docs.aws.amazon.com/AmazonVPC/latest/PeeringGuide/Welcome.html">inter-region VPC peering</a>. This would allow us to connect to our us-west-1 region and access <a href="https://aws.amazon.com/rds/">RDS</a> and <a href="https://aws.amazon.com/kms/">KMS</a>.</p>
<p><img src="/static/images/migrate-k8s-005.png" alt="Inter Region peering between us-west-1 and us-west-2"><em>Inter Region peering between us-west-1 and us-west-2</em></p>
<p>This too set us back. As it turns out, the <em>us-west-1</em> region isn’t the best region you could use. At the time we investigated this, <em>us-west-1</em> didn’t support inter-region VPC peering. This means that we couldn’t use this solution either.</p>
<h2 id="decisions-and-compromises">Decisions and compromises</h2>
<p>With all this new knowledge, it was time to make a decision. Would we stay with AWS or move over to another provider?</p>
<p>It’s worth noting that moving to another provider comes with a lot of extra overhead as well. We’d have to expose our database, migrate our KMS and re-encrypt all our data.</p>
<p>In the end, we decided to <strong>stick with AWS</strong> and run with the tolerating node failure solution. With <a href="https://aws.amazon.com/eks/">the announcement of Amazon EKS</a> and inter-region peering coming soon, we felt like this was a good enough first step.</p>
<p>Managing your own cluster can be time consuming. To date, we’ve seen minimal impact, but we definitely <strong>accounted for cluster maintenance</strong>. The most time consuming would be <strong>cluster updates</strong>.</p>
<p>From a financial standpoint, we also compromised. Yes, it’d be cheaper than the legacy setup, but it’d be <strong>more expensive than the competitors</strong>. Azure and GCP both provide the master nodes for free, which cuts down in cost quite a bit.</p>
<h2 id="kops-tips">kops tips</h2>
<p>For us, kops has worked great. It does come with a set of defaults that you should be aware of and overwrite. One of the key things to do is to <strong>enable etcd encryption</strong>. This is done by providing the* — encrypt-etcd-storage* flag.</p>
<p>By default, kops also doesn’t <strong>enable RBAC</strong>. <a href="https://kubernetes.io/docs/admin/authorization/rbac/">RBAC</a> is a great mechanism to limit the scope of your applications within your cluster. We highly recommend enabling this and setting up appropriate roles.</p>
<p>For security reasons, we’ve <strong>disabled SSH</strong> into our instances. This ensures that no one can access these boxes, even when we run with a <strong>private network topology</strong>.</p>
<h2 id="configuring-the-cluster">Configuring the cluster</h2>
<p>With a cluster up and running, it’s time to get to work. The next step would be to configure it so that we can start deploying applications into it.</p>
<p>Having managed our services with <a href="https://www.terraform.io/">Terraform</a> before meant that we had quite a bit of control on how to set things up. We managed our ELBs, DNS, logging etc. through our Terraform configuration. We needed to make sure we can do this with our Kubernetes setup as well.</p>
<h2 id="load-balancing">Load Balancing</h2>
<p>Kubernetes has the notion of <a href="https://kubernetes.io/docs/concepts/services-networking/service/">Services</a> and <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">Ingresses</a>. With a Service, it’s possible to group pods — usually managed by a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployment</a> — and expose them under the same endpoint. This endpoint could either be internal or external. When configuring a Service as a <a href="https://en.wikipedia.org/wiki/Load_balancing_%28computing%29">LoadBalancer</a>, Kubernetes will generate an ELB. This ELB is then linked to the configured service.</p>
<p><img src="/static/images/migrate-k8s-006.png" alt="Service LoadBalancer"><em>Service LoadBalancer</em></p>
<p>This is great, but there are <a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-limits.html">limits on the amount of ELBs</a> you can have. By using an Ingress, we can create a single ELB and route the traffic within our cluster. There are several Ingresses available, but we went with the default <a href="https://github.com/kubernetes/ingress-nginx">Nginx Ingress</a>.</p>
<p><img src="/static/images/migrate-k8s-007.png" alt="Ingress LoadBalancer"><em>Ingress LoadBalancer</em></p>
<p>Now that we can route traffic to a service, it’s time to expose these through a domain. To do this, we used the <a href="https://github.com/kubernetes-incubator/external-dns">External DNS</a> project. This is a great way to keep the configured domain names close to your application.</p>
<p>The last step we had to look at for exposing our services was making sure we served traffic through SSL. This turned out to be easy as well as there are already available solutions to this. We settled on <a href="https://github.com/jetstack/cert-manager">cert-manager</a>, which integrates with our Nginx Ingress.</p>
<h2 id="service-configuration">Service Configuration</h2>
<p>Service Configuration was an easy win for us. We already started building Manifold on top of Manifold with our <a href="https://github.com/manifoldco/terraform-provider-manifold/">Terraform Integration</a>. Because of this, all the credentials we needed were already configured.</p>
<p>We designed our <a href="https://github.com/manifoldco/kubernetes-credentials">Kubernetes Integration</a> with our Terraform Integration in mind. We kept the underlying semantics the same which meant that migrating credentials was a breeze.</p>
<p>We also added the <a href="https://github.com/manifoldco/kubernetes-credentials/pull/20">option for custom secret types</a>. This allowed us to configure a <a href="https://hackernoon.com/tagged/docker">Docker</a> Auth secret. When <a href="https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/">pulling Docker images from a private registry</a>, you need this secret.</p>
<h2 id="telemetry">Telemetry</h2>
<p>One of the most important things of running a distributed system is knowing what’s going on inside of it. For this, you need to set up centralized logging and metrics. We had this in our legacy platform so we definitely needed this in our new platform.</p>
<p>For logging, we expanded our dogfooding and set up <a href="https://www.manifold.co/services/logdna">our LogDNA Integration</a> to gather logs. <a href="https://logdna.com/">LogDNA</a> themselves provide a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/">DaemonSet</a> <a href="https://docs.logdna.com/docs/kubernetes">configuration</a>. This allows you to ship the logs from your cluster to their platform.</p>
<p>For metrics, we were relying on <a href="https://www.datadoghq.com/lpg/?utm_source=Advertisement&amp;utm_medium=GoogleAdsNon1stTierBrand&amp;utm_campaign=GoogleAdsNon1stTierBrand-Non1st&amp;utm_content=Datadog&amp;utm_keyword=%7Bkeyword%7D&amp;utm_matchtype=%7Bmatchtype%7D&amp;gclid=EAIaIQobChMIyviblJDO2QIVFzobCh2sdQtzEAAYASAAEgLUA_D_BwE">Datadog</a> which worked well for us so far. As with LogDNA, Datadog also <a href="https://docs.datadoghq.com/agent/basic_agent_usage/kubernetes/">provides a DaemonSet configuration</a>. They even have a <a href="https://www.datadoghq.com/blog/monitor-kubernetes-docker/">great blog post</a> on how to set this up!</p>
<h2 id="migration">Migration</h2>
<p>With the cluster configured and our applications deployed, it was time to migrate. To <strong>ensure zero down time</strong>, we had to do this in several stages.</p>
<p>The first stage was running the cluster on a** separate domain**. By connecting the two systems, we could test it without interrupting anyone. This helped us find and fix some early stage issues.</p>
<p>In the next stage, we’d <strong>route some traffic to the Kubernetes cluster</strong>. To do this, we set up <a href="https://en.wikipedia.org/wiki/Round-robin_DNS">round robin</a>. This is a great way to see how your cluster behaves with actual traffic. After about a week, we had enough confidence to move on to the next phase.</p>
<p><img src="/static/images/migrate-k8s-008.gif" alt="DNS round-robin between the Legacy infrastructure and our Kubernetes cluster"><em>DNS round-robin between the Legacy infrastructure and our Kubernetes cluster</em></p>
<p>The third stage involved <strong>removing the legacy DNS records</strong>. After removing the appropriate Terraform configuration, all our traffic would flow through Kubernetes!</p>
<p>Because of DNS cache, we decided to keep the legacy up and running for a few more days. This way, people with a cached DNS entry would not encounter an error. This also gave us the possibility to rollback in case we saw something go wrong.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Now that we’ve migrated, we can reflect back on things. Our team — and the company — has called this migration a success. We <strong>decreased our deployment time from ~15min to ~1.5min</strong> and managed to <strong>cut operational costs</strong> by doing so.</p>
<p>We still <strong>haven’t finished up our Continuous Delivery</strong> pipeline, but we’re working on it. We’ve started work on <a href="https://heighliner.com/">Heighliner</a> which will in the first place help ourselves, but hopefully help others as well.</p>
<p>We encountered one major setback: not having 3 availability zones available to us. This <strong>prevented us from running Highly Available across zones</strong>. It’s a compromise we decided to make to get us up and running and we’ll be looking at fixing that soon.</p>
<p>Oh, and one more thing. <strong>So. Much. YAML</strong>. This is where <a href="https://heighliner.com/">Heighliner</a> will help us out.</p>
<p>Massive shout-out to <a href="https://medium.com/@megthesmith">Meg Smith</a> for her work on the images for this blog post.</p>
<!-- raw HTML omitted -->

            </div>
        </article>

        <hr />

        <div class="post-info">
            
            
  		</div>
    </main>

            </div>

            
                <footer class="footer">
    <div class="footer__inner">
        <div class="footer__content">
            <span>&copy; 2020</span>
            
                <span><a href="/">Jelmer Snoeck</a></span>
            
            
            <span> <a href="/posts/index.xml" target="_blank" title="rss"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 20 20" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-rss"><path d="M4 11a9 9 0 0 1 9 9"></path><path d="M4 4a16 16 0 0 1 16 16"></path><circle cx="5" cy="19" r="1"></circle></svg></a></span>
        </div>
    </div>
    <div class="footer__inner">
        <div class="footer__content">
            <span>Powered by <a href="http://gohugo.io">Hugo</a></span>
            <span>Made with &#10084; by <a href="https://github.com/rhazdon">Djordje Atlialp</a></span>
          </div>
    </div>
</footer>

            
        </div>

        




<script type="text/javascript" src="/bundle.min.dc716e9092c9820b77f96da294d0120aeeb189b5bcea9752309ebea27fd53bbe6b13cffb2aca8ecf32525647ceb7001f76091de4199ac5a3caa432c070247f5b.js" integrity="sha512-3HFukJLJggt3&#43;W2ilNASCu6xibW86pdSMJ6&#43;on/VO75rE8/7KsqOzzJSVkfOtwAfdgkd5BmaxaPKpDLAcCR/Ww=="></script>
    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-53955110-2', 'auto');
	
	ga('send', 'pageview');
}
</script>




    </body>
</html>
